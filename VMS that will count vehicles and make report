import cv2
import numpy as np
import time
from collections import defaultdict

# Load YOLO model and class labels
net = cv2.dnn.readNet("yolov3.weights", "yolov3.cfg")
with open("coco.names", "r") as f:
    classes = f.read().strip().split("\n")

# Initialize object tracker
tracker = cv2.TrackerCSRT_create()

# Open video capture (0 for camera or specify video file)
cap = cv2.VideoCapture(0)

# Vehicle tracking dictionary (to store trackers for each vehicle)
vehicle_trackers = {}

# Variables for vehicle counting and speed calculation
vehicle_count = 0
previous_frame_time = time.time()
previous_positions = defaultdict(list)

while True:
    ret, frame = cap.read()

    if not ret:
        break

    # Preprocess the frame for YOLO input requirements
    blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416, 416), swapRB=True, crop=False)
    net.setInput(blob)

    # Get output layer names
    output_layer_names = net.getUnconnectedOutLayersNames()

    # Run forward pass
    layer_outputs = net.forward(output_layer_names)

    # Initialize variables for vehicle detection
    detected_vehicles = []

    for output in layer_outputs:
        for detection in output:
            scores = detection[5:]
            class_id = np.argmax(scores)
            confidence = scores[class_id]

            if confidence > 0.5 and classes[class_id] == "car":
                center_x, center_y, w, h = map(int, detection[0:4] * [frame.shape[1], frame.shape[0], frame.shape[1], frame.shape[0]])
                x, y = int(center_x - w / 2), int(center_y - h / 2)
                detected_vehicles.append((x, y, w, h))

    # Update vehicle trackers
    for vehicle_id in list(vehicle_trackers.keys()):
        success, box = vehicle_trackers[vehicle_id].update(frame)

        if success:
            x, y, w, h = [int(i) for i in box]

            # Calculate vehicle speed
            current_time = time.time()
            time_elapsed = current_time - previous_frame_time

            if time_elapsed > 0:
                x_center = x + w / 2
                y_center = y + h / 2
                previous_positions[vehicle_id].append((x_center, y_center, current_time))

                # Calculate speed based on distance and time
                if len(previous_positions[vehicle_id]) > 1:
                    prev_x, prev_y, prev_time = previous_positions[vehicle_id][-2]
                    distance = np.sqrt((x_center - prev_x)**2 + (y_center - prev_y)**2)
                    speed = distance / time_elapsed  # Speed in pixels per second
                    print(f"Vehicle ID: {vehicle_id}, Speed: {speed:.2f} px/s")

            previous_frame_time = current_time
        else:
            del vehicle_trackers[vehicle_id]

    # Match detected vehicles to existing trackers or create new ones
    for detected_vehicle in detected_vehicles:
        x, y, w, h = detected_vehicle
        matched = False

        for vehicle_id, tracker in vehicle_trackers.items():
            success, box = tracker.update(frame)

            if success:
                x_t, y_t, w_t, h_t = [int(i) for i in box]

                if x < x_t + w_t and x + w > x_t and y < y_t + h_t and y + h > y_t:
                    matched = True
                    break

        if not matched:
            tracker = cv2.TrackerCSRT_create()
            tracker.init(frame, (x, y, w, h))
            vehicle_trackers[len(vehicle_trackers)] = tracker
            vehicle_count += 1

    # Display the frame
    cv2.imshow("Vehicle Detection and Tracking", frame)

    # Exit when the 'q' key is pressed
    if cv2.waitKey(1) & 0xFF == ord("q"):
        break

# Release video capture and close all OpenCV windows
cap.release()
cv2.destroyAllWindows()
